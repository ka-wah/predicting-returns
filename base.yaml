data:
  path: data/processed/options_with_returns_scaled.csv
  date_col: date
  target: ret_dh_w_adj 
  target_scale:
    kind: none        # "none", "vega", or "gamma"
    abs: true        # use absolute value of the Greek when scaling (recommended)
    floor: 1e-6      # floor for the scale to avoid division by ~0
  features: [
    # Contract/term structure (levels)
    "corp","moneyness","tau","dte","dte_1","dte_2","dte_3p",

    # Greeks & convexity (levels; already scaled where relevant)
    "delta_model","vega_model","theta_model","gamma_scaled",

    # IV structure (levels) + short-horizon change
    "atm_iv","atm_iv_chg","atm_term_slope","smile_slope",

    # IV–RV alignment (levels) and interactions
    "rv_ann_past","ivrv_pred","ivrv_ratio_pred","convexity_proxy","vega_ivchg_proxy",

    # Option microstructure at t (levels)
    "opt_spread_hi",

    # Underlying microstructure & volatility (Δ and z; minimal levels)
    "baspread_chg","baspread_z",
    "rv_chg","rv_z",
    "pfb","pfb_z",
    "indmom_z",
    "realskew_chg","realskew_z",

    # Attention/sentiment (Δ and z; drop raw levels)
    "public_chg","public_z",
    "rtrs_pos_chg","rtrs_pos_z","rtrs_neg_chg","rtrs_neg_z","rtrs_neu_chg","rtrs_neu_z","rtrs_total_chg","rtrs_total_z",
    "reddit_pos_chg","reddit_pos_z","reddit_neg_chg","reddit_neg_z","reddit_neu_chg","reddit_neu_z","reddit_total_chg","reddit_total_z"
  ]
  dropna: false
  atm_band: 0.03

  # Side filter (optional). Set to "call" or "put" to run one side only; leave empty/omit for both.
  side:
  cp_col: cp
  call_values: [1, "C"]
  put_values: [0, -1, "P"]

diagnostics:
  generic: true
  linear: false
  topk_scatter: 8

split:
  train_frac: 0.70
  val_frac: 0.15
  period: D

forecast:
  horizon_periods: 1

train:
  models: ["ols", "ridge_cv", "enet_pairs", "huber", "lgbm_gbdt", "lgbm_dart", "torch_ffn", "pcr", "pls", "rf", "zero"]
  refit_on: "train_val"
  random_state: 42
  # target processing
  y_winsor_p: 0.00
  y_transform: "yeo-johnson"     # or "asinh" or ""
  y_asinh_scale: 0.5
  # feature coverage filter (TRAIN-only)
  min_feature_coverage: 0.90

# Random-search spaces (only models listed here will be tuned and get cv_random_*.csv)
tuning:
  method: random
  n_iter: 5
  cv:
    n_splits: 1
    min_train_periods: 6
    val_periods: 1
    period: "W-FRI"
    gap: "0W"
  select:
    rule: one_se         # keep this
    tie_breaker: r2_oos  # keep this
  # spaces:  # keep only where you want to override defaults

  spaces:
    torch_ffn:
      hidden_dims:
        values:
          - [32]
          - [64]
          - [128]
          - [32, 32]
          - [64, 64]
          - [128, 128]
          - [32, 32, 32]
          - [64, 64, 64]
          - [128, 128, 128]

      activation: { values: ["relu", "tanh", "gelu"] }

      dropout:    { dist: uniform,    low: 0.0,    high: 0.25 }

      # Learning rate and weight decay (L2) – log/linear ranges
      lr:           { dist: loguniform, low: 5.0e-4, high: 5.0e-2 }
      weight_decay: { dist: loguniform, low: 1.0e-6, high: 1.0e-3 }

      # Training budget
      epochs:     { dist: int_uniform, low: 30, high: 100 }
      patience:   { values: [8, 12, 16] }

      # Batch size – keep modest on small data / laptop GPU
      batch_size: { values: [512, 1024, 2048] }

      # Misc
      seed:       { values: [42] }

    lgbm_gbdt:
      n_estimators:        {dist: int_loguniform, low: 200,  high: 4000}
      learning_rate:       {dist: loguniform,     low: 0.02, high: 0.10}
      max_depth:           {dist: int_uniform,    low: 4,    high: 10}
      num_leaves:          {dist: int_uniform,    low: 15,   high: 127}
      min_child_samples:   {dist: int_uniform,    low: 50,   high: 300}
      feature_fraction:    {values: [0.7, 0.8, 0.9, 1.0]}
      bagging_fraction:    {values: [0.7, 0.8, 0.9]}
      bagging_freq:        {values: [1, 3, 5]}
      reg_alpha:           {values: [0.0, 0.1, 0.5, 1.0]}
      reg_lambda:          {values: [0.0, 1.0, 10.0, 50.0]}
      min_split_gain:      {values: [0.0, 0.01]}
      verbose:             {values: [-1]}
      random_state:        {values: [42]}

    lgbm_dart:
      n_estimators:        {dist: int_loguniform, low: 400,  high: 6000}
      learning_rate:       {dist: loguniform,     low: 0.02, high: 0.10}
      max_depth:           {dist: int_uniform,    low: 4,    high: 10}
      num_leaves:          {dist: int_uniform,    low: 15,   high: 127}
      min_child_samples:   {dist: int_uniform,    low: 50,   high: 300}
      feature_fraction:    {values: [0.7, 0.8, 0.9, 1.0]}
      bagging_fraction:    {values: [0.7, 0.8, 0.9]}
      bagging_freq:        {values: [1, 3, 5]}
      reg_alpha:           {values: [0.0, 0.1, 0.5, 1.0]}
      reg_lambda:          {values: [0.0, 1.0, 10.0, 50.0]}
      min_split_gain:      {values: [0.0, 0.01]}
      drop_rate:           {values: [0.05, 0.10, 0.15]}
      skip_drop:           {values: [0.25, 0.50]}
      verbose:             {values: [-1]}
      random_state:        {values: [42]}

    # ================= Random Forest (trees) =================
    random_forest:
      n_estimators:     {values: [300, 600, 1000]}
      max_depth:        {values: [null, 6, 10, 15]}
      min_samples_leaf: {values: [1, 2, 5, 10]}
      max_features:     {values: ["sqrt", "log2", 0.5]}
      bootstrap:        {values: [true]}

    # ================= Elastic Net (with interactions) =================
    enet_pairs:
      top_k:        {dist: int_uniform, low: 16,  high: 128}
      min_abs_corr: {dist: uniform,     low: 0.05, high: 0.10}

      # Robust ENetCV path (avoid ultra-tiny alphas)
      l1_ratio:     {values: [0.2, 0.4, 0.6, 0.8]}
      n_alphas:     {values: [60]}          # dense enough
      eps:          {values: [0.001]}       # floor alpha_min at 0.1% of alpha_max

      max_iter:     {values: [200000]}
      tol:          {values: [1.0e-4, 3.0e-4]}
      selection:    {values: ["random"]}    # faster, more stable on wide X
      fit_intercept: {values: [True]}
      random_state: {values: [42]}


    # ================= Ridge (random-tuned; use model name "ridge") =================
    ridge:
      alpha:         {dist: loguniform, low: 1.0e-6, high: 1.0e-2}
      fit_intercept: {values: [true]}
    # ================= PCR (standardize outside; disable internal scaling) =================
    pcr:
      n_components:  {values: [1,2,3,4,5,6]}   # cap ≤ your feature count
      standardize:   {values: [true]}
      whiten:        {values: [false]}
      svd_solver:    {values: ["auto"]}
      reg:           {values: ["ols"]}                        # or ["ols","ridge"]
      # alpha only used if reg=="ridge"
      # alpha:         {dist: loguniform, low: 1.0e-6, high: 1.0e-2}

    # ----- PLS (standardize outside; disable internal scaling) -----
    pls:
      n_components:  {values: [1,2,3,4,5,6]}
      standardize:   {values: [true]}
      max_iter:      {values: [500, 1000]}
      tol:           {values: [1.0e-6, 1.0e-5]}
      imputer_strategy: {values: ["mean"]}


model_params:
  ols:
    add_const: true
    nw_lags: 0

  # RidgeCV uses sklearn internal CV (not your tuner)
  ridge_cv:
    alphas: [0.001, 0.01, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000]
    cv: 5
    scoring: neg_mean_absolute_error
    imputer_strategy: mean

  enet_pairs:
    add_original: true
    max_pairs: 3000
    top_k: 150
    min_abs_corr: 0.02
    verbose: false
    cv: 5
    # Explicit groups per column (safe if some cols not present)
    group_map:
      moneyness: I
      tau: I
      iv: V
      atm_iv: V
      rv_ann_past: R
      ivrv_pred: R
      ivrv_ratio_pred: R
      delta_model: G
      vega_model: G
      theta_model: G
      gamma_scaled: G
      F: S
      baspread: S
      rv: S
      pfb: S
      indmom: S
      realskew: S
    allowed_pairs:
      - [I, S]
      - [V, S]
      - [R, S]
      - [I, G]
      - [V, G]
      - [R, G]

  lgbm_gbdt:
    n_jobs: -1
    # device: gpu
    objective: mse
    metric: mse
    random_state: 42

  lgbm_dart:
    n_jobs: -1
    # device: gpu
    objective: mse
    metric: mse
    boosting_type: dart
    random_state: 42

  torch_ffn:
    hidden_dims: [256, 256, 128]
    dropout: 0.10
    activation: relu
    epochs: 80
    batch_size: 2048        # adjust down if OOM or too small dataset
    lr: 0.001
    weight_decay: 0.0001
    patience: 12
    device: "auto"          # or omit to auto-detect CUDA
    verbose: true
    amp: true               # if you added AMP per my earlier snippet

benchmark: "zero"

outputs:
  dir: results/experiments/new-grid
  predictions_csv: predictions_test.csv
  metrics_csv: metrics_test.csv
  cv_dir: cv
  coeffs_ols_csv: coefficients_ols.csv
  coeffs_ridge_csv: coefficients_ridge.csv
  coeffs_enet_pairs_csv: coefficients_enet_pairs.csv
  coeffs_enet_pairs_orig_csv: coefficients_enet_pairs_orig.csv
  enet_cv_curve_csv: enet_cv_curve.csv
  enet_summary_json: enet_summary.json
  index_label: row_id
